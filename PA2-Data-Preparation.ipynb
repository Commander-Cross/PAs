{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322](https://github.com/GonzagaCPSC322) Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "## PA2 Data Preparation (100 pts)\n",
    "\n",
    "## Learner Objectives\n",
    "At the conclusion of this programming assignment, participants should be able to:\n",
    "* Model a 2D table with a custom class\n",
    "* Implement common data preparation tasks, such as\n",
    "    * Identify and remove duplicates \n",
    "    * Join tables\n",
    "    * Compute simple summary statistics\n",
    "    * Handle missing values\n",
    "\n",
    "## Prerequisites\n",
    "Before starting this programming assignment, participants should be able to:\n",
    "* Understand terms and concepts in Chapters 1 and 2 of Bramer\n",
    "* Run Python code and tests in Docker containers\n",
    "* Work with files, functions, and 2D lists in Python\n",
    "\n",
    "## Acknowledgments\n",
    "Content used in this assignment is based upon information in the following sources:\n",
    "* Part 2 (auto dataset preparation): Dr. Shawn Bowers' Data Mining HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Classroom Setup\n",
    "For this assignment, you will use GitHub Classroom to create a private code repository to track code changes and submit your assignment. Open this PA2 link to accept the assignment and create a private repository for your assignment in Github classroom: https://classroom.github.com/a/pJolQbYq\n",
    "\n",
    "Your repo, for example, will be named GonzagaCPSC322/pa2-yourusername (where yourusername is your Github username). I highly recommend committing/pushing regularly so your work is always backed up. We will grade your most recent commit, even if that commit is after the due date (your work will be marked late if this is the case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Requirements\n",
    "This assignment involves implementing common data preparation algorithms. It has two main parts:\n",
    "1. MyPyTable (`mypytable.py`): Finish a class called `MyPyTable` that represents a 2D table of data. \n",
    "1. Auto Dataset Preparation (`pa2.py`): Write a program that uses `MyPyTable` to perform data preparation tasks on an automobile dataset.\n",
    "\n",
    "Note: we are learning data science from scratch! The only non-standard Python library you should need to use for this assignment is `tabulate` (if you'd like to use it for pretty printing). You can install `tabulate` in your Docker container at the command line with `pip install tabulate`. This means that beyond `tabulate`, you should not `pip install` any additional libraries beyond what is included in the [continuumio/anaconda3:2022.05](https://hub.docker.com/r/continuumio/anaconda3) Docker image and you should not use `pandas/numpy/scipy/`etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: `MyPyTable` (55 pts)\n",
    "### Step 1: Familiarize Yourself with `mypytable.py`\n",
    "Read through the provided `mypytable.py` module to understand the state and behavior of a `MyPyTable`. Read the method docstrings carefully.\n",
    "\n",
    "### Step 2: Implement and Test `MyPyTable` Methods\n",
    "For each `MyPyTable` method with a `TODO`:\n",
    "1. Implement the method according to the method's docstring\n",
    "1. \"Unofficially\" test your implementation by writing your own code to call that method with various inputs\n",
    "    * You can do this at the bottom of `mypytable.py` in a `if __name__ == \"__main__\":` condition, or you can do this in a new Python module you make\n",
    "    * Note: Because it can be difficult to understand failing unit test output, I strongly encourage you to do this step BEFORE running the provided unit tests (next step)\n",
    "1. \"Officially\" test your method for functional correctness by running the unit tests in `test_mypytable.py`\n",
    "\n",
    "Note: passing the inner and outer join tests are the most challenging parts of this assignment. Please make sure you complete the \"Join Practice Problem\" given in class and you understand the tracing both algorithms on all examples before starting the joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ðŸš— Auto Dataset Preparation ðŸš— (35 pts)\n",
    "Write a program (`pa2.py`) that uses `MyPyTable` objects to prepare the [auto-mpg.txt and auto-prices.txt files](https://github.com/GonzagaCPSC322/PAs/tree/master/files) for future data mining tasks. These files contain information about cars manufactured and sold in the 1970s.\n",
    "\n",
    "The attributes of `auto-mpg.txt` are: \n",
    "* mpg (miles per gallon)\n",
    "* cylinders\n",
    "* displacement\n",
    "* horsepower\n",
    "* weight\n",
    "* acceleration\n",
    "* model year\n",
    "* origin (1=US, 2=Europe, 3=Japan)\n",
    "* car name\n",
    "\n",
    "The attributes of `auto-prices.txt` are: \n",
    "* car name\n",
    "* model year\n",
    "* msrp (manufacturer's suggested retail price)\n",
    "\n",
    "Notes for the following Part 2 Steps:\n",
    "* Code for all of these steps should be written in `pa2.py` and use `MyPyTable` objects appropriately.\n",
    "* Take notes of the process you use to \"clean\" the given data in comment blocks.\n",
    "* Design functions that are generic and re-usable for future programming assignments and data mining tasks.\n",
    "* Write files to a folder called `output_data`. It is good practice to keep your source code files separated from your data files.\n",
    "* Read in the txt files from the `input_data` folder \n",
    "* Program output does not need to be formatted to match the examples below, but should still be formatted to be easily readable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Duplicates (5 pts)\n",
    "For each dataset, display the number of instances in the dataset and display any duplicates (i.e., instances with the same car name and model year). If duplicates do exist, programmatically resolve the duplicates and write the modified dataset to a file (see filenames in the example output below). Write in a comment block the duplicates you found (if any), how you resolved them, and why you resolved them the way you did. The result of this step should print the following (where ??? should be replaced with the actual number of instances):\n",
    "\n",
    "```\n",
    "--------------------------------------------------\n",
    "auto-mpg.txt:\n",
    "--------------------------------------------------\n",
    "No. of instances: ???\n",
    "Duplicates: ???\n",
    "--------------------------------------------------\n",
    "auto-prices.txt:\n",
    "--------------------------------------------------\n",
    "No. of instances: ???\n",
    "Duplicates: ???\n",
    "\n",
    "--------------------------------------------------\n",
    "duplicates removed (saved as auto-mpg-nodups.txt):\n",
    "--------------------------------------------------\n",
    "No. of instances: ???\n",
    "Duplicates: []\n",
    "--------------------------------------------------\n",
    "duplicates removed (saved auto-prices-nodups.txt):\n",
    "--------------------------------------------------\n",
    "No. of instances: ???\n",
    "Duplicates: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Manual Clean (10 pts)\n",
    "Perform a full outer join between your mpg and prices tables that have duplicates removed. Inspect the result of this join to find cases where there are prices but no mpg data. We want to manually (i.e. not with Python) clean up the cases in which these mismatches occur. An example of a mismatch is the audi 100 ls. In the full outer joined table there will be the row: `NA,NA,NA,NA,NA,NA,71,NA,audi 100 ls,3595.0` (note: it is okay if your model year is a float). In the mpg table, there is a row: `24.0,4.0,107.0,90.0,2430.0,14.5,70,2.0,audi 100 ls` and in the prices table there is a row: `audi 100 ls,71,3595.0`. These two rows should match up on car name and model year, but the year is incorrect in the prices table and should be manually corrected so we have a match on full outer join.\n",
    "\n",
    "To do make these corrections, manually (e.g. with `cp` at command line or using a graphical user interface such as Windows File Explorer or Mac Finder) make a copy of `auto-mpg-nodups.txt` called `auto-mpg-clean.txt` and a copy of `auto-prices-nodups.txt` called `auto-prices-clean.txt`. Put these copies in the `input_data` folder. Open these files in a text editor (e.g. VS Code) and manually edit them as necessary to resolve the issues (e.g., in the case of a mis-matched year like above, or something different like a misspelling). Write in a comment block in your `pa2.py` how and why you resolved these cases the way you did.\n",
    "\n",
    "For each dataset, count the number of instances (as in Step 1 above). The result of this step should print the following:\n",
    "\n",
    "```\n",
    "--------------------------------------------------\n",
    "auto-mpg-clean.txt:\n",
    "--------------------------------------------------\n",
    "No. of instances: ???\n",
    "Duplicates: []\n",
    "--------------------------------------------------\n",
    "auto-prices-clean.txt:\n",
    "--------------------------------------------------\n",
    "No. of instances: ???\n",
    "Duplicates: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Join (5 pts)\n",
    "Combine the two datasets on car name and model year using a *full outer join*. The resulting joined table should have 10 attributes such that the first 9 attributes are those from `auto-mpg-clean.txt` and the last attribute is the corresponding msrp (price) from `auto-prices-clean.txt`.\n",
    "\n",
    "Write out the result to `auto-data.txt` and count the number of instances (as in Step 1 above). The result of this step should print the following:\n",
    "\n",
    "```\n",
    "--------------------------------------------------\n",
    "combined table (saved as auto-data.txt):\n",
    "--------------------------------------------------\n",
    "No. of instances: ???\n",
    "Duplicates: []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Summary Stats (5 pts)\n",
    "Compute summary statistics for auto-data.txt. For each continuous attribute, compute the minimum, maximum, midpoint (half way between min and max), average, and median values. You can ignore the categorical attributes for now. The result of this step should print the following:\n",
    "\n",
    "```\n",
    "Summary Stats:\n",
    "attribute       min      max       mid        avg    median\n",
    "------------  -----  -------  --------  ---------  --------\n",
    "mpg             ???      ???       ???        ???       ???\n",
    "displacement    ???      ???       ???        ???       ???\n",
    "horsepower      ???      ???       ???        ???       ???\n",
    "weight          ???      ???       ???        ???       ???\n",
    "acceleration    ???      ???       ???        ???       ???\n",
    "model year      ???      ???       ???        ???       ???\n",
    "msrp            ???      ???       ???        ???       ???\n",
    "```\n",
    "\n",
    "Note: the table above is nicely generated using the [`tabulate`](https://pypi.org/project/tabulate/) module (which you can install in your Docker container at the command line with `pip install tabulate`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Missing Values (10 pts)\n",
    "Perform two different techniques to resolve missing values in `auto-data.txt`. Note that there should only be three columns in `auto-data.txt` that contain missing values. \n",
    "1. The first approach should be to remove all instances with missing values.\n",
    "2. The second approach should be to replace missing values with their corresponding attribute's average value.\n",
    "\n",
    "After each approach, count the number of instances and display summary statistics (like Step 4) on the data with missing values resolved. Write the modified datasets to a file (see filenames in the example output below). The result of this step should print the following:\n",
    "\n",
    "```\n",
    "--------------------------------------------------\n",
    "combined table - rows w/missing values removed (saved as auto-data-removed-NA.txt)\n",
    "--------------------------------------------------\n",
    "No. of instances: ???\n",
    "Duplicates: []\n",
    "Summary Stats:\n",
    "attribute       min      max       mid        avg    median\n",
    "------------  -----  -------  --------  ---------  --------\n",
    "...\n",
    "\n",
    "--------------------------------------------------\n",
    "combined table - rows w/missing values replaced (saved as auto-data-replaced-NA.txt)\n",
    "--------------------------------------------------\n",
    "No. of instances: ???\n",
    "Duplicates: []\n",
    "Summary Stats:\n",
    "attribute       min      max       mid        avg    median\n",
    "------------  -----  -------  --------  ---------  --------\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus (6 pts)\n",
    "* (2 pts) Run the `pylint` linter over your PA2 code and make sure it receives a 10/10 rating\n",
    "    * See PA1 bonus for more information on how to do this.\n",
    "* (2 pts) Add a third approach to `MyPyTable` for resolving missing values for Step 5. This approach should also replace missing values with average values (like approach #2 in Step 5), but based on meaningful subsets of the data, e.g., based on the model year, origin, or some combination of attributes that makes the most sense to you. Document your decisions in a comment block in your code.\n",
    "* (2 pts) Change your program so the names of the two input files will be passed in to your program via [command line arguments](https://docs.python.org/3/tutorial/stdlib.html#command-line-arguments). Use the names of these files to programmatically create subsequent file names. \n",
    "\n",
    "For example, running your program as follows: \n",
    "```\n",
    "python pa2.py auto-mpg.txt auto-prices.txt\n",
    "```\n",
    "\n",
    "would produce the same file names as listed in the specifications above. But if the names of the files are car-mpg.txt and car-prices.txt, then running your program as follows:\n",
    "```\n",
    "python pa2.py car-mpg.txt car-prices.txt\n",
    "```\n",
    "\n",
    "would produce file names such as car-mpg-nodups.txt, car-prices-nodups.txt, etc. Essentially, you are not hard-coding filenames in your program but instead constructing them at runtime using the command line arguments.\n",
    "\n",
    "If incorrect command line arguments are given (e.g. missing one of them), print a string showing usage instructions. This helps the user know how to run your program!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Assignments\n",
    "1. Use Github classroom to submit your assignment via a Github repo. See the \"Github Classroom Setup\" section at the beginning of this document for details on how to do this. You must commit your solution by the due date and time.\n",
    "1. Double check your submission by \"pretending to be the grader\": clone (or download a zip) your submission repo and run your code in a fresh [continuumio/anaconda3:2022.05](https://hub.docker.com/r/continuumio/anaconda3) Docker container like we will when we grade your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Guidelines\n",
    "This assignment is worth 100 points + 6 points bonus. Your assignment will be evaluated based on a successful execution in the [continuumio/anaconda3:2022.05](https://hub.docker.com/r/continuumio/anaconda3) Docker container and adherence to the program requirements. We will grade according to the following criteria:\n",
    "* 55 pts for passing all `MyPyTable` unit tests\n",
    "    * 3 pts for test_get_shape\n",
    "    * 3 pts for test_get_column\n",
    "    * 3 pts for test_convert_to_numeric\n",
    "    * 3 pts for test_drop_rows\n",
    "    * 3 pts for test_load_from_file\n",
    "    * 5 pts for test_save_to_file\n",
    "    * 5 pts for test_find_duplicates\n",
    "    * 5 pts for test_remove_rows_with_missing_values\n",
    "    * 5 pts for test_replace_missing_values_with_column_average\n",
    "    * 6 pts for test_compute_summary_statistics\n",
    "    * 6 pts for test_perform_inner_join\n",
    "    * 8 pts for test_perform_full_outer_join\n",
    "* 5 pts for correct step 1\n",
    "* 10 pts for correct step 2\n",
    "* 5 pts for correct step 3\n",
    "* 5 pts for correct step 4\n",
    "* 10 pts for correct step 5\n",
    "* 10 pts for adherence to course [coding standard](https://nbviewer.jupyter.org/github/GonzagaCPSC322/PAs/blob/master/Coding%20Standard.ipynb), including clearly documenting data pre-processing decisions in comment blocks."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
